{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ca3d61-c88a-4eac-bc4c-005d2acc66fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a2b19d-0d17-4a2f-8e87-abbf2c5a4bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(28 * 28 * 1, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "  \n",
    "    def compute_l1_loss(self, w):\n",
    "        return torch.abs(w).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5778e-f2da-49fb-ac90-b7b4e0f74fb6",
   "metadata": {},
   "source": [
    "**L1 Regularization**, also called **Lasso Regularization**, involves adding the absolute value of all weights to the loss value.\n",
    "\n",
    "Suppose that you are using binary crossentropy loss with your PyTorch based classifier. You want to implement L1 Regularization, which effectively involves that $\\sum_f{ _{i=1}^{n}} | w_i |$ is added to the loss.\n",
    "\n",
    "Here, [latex]n[/latex] represents the number of individual weights, and you can see that we iterate over these weights. We then take the absolute value for each value $w_i$ and sum everything together.\n",
    "\n",
    "In other words, L1 Regularization loss can be implemented as follows:\n",
    "\n",
    "$\\text{full_loss = original_loss + } \\sum_f{ _{i=1}^{n}} | w_i |$\n",
    "\n",
    "Here, original_loss is binary crossentropy. However, it can be pretty much any loss function that you desire!\n",
    "\n",
    "Implementing L1 Regularization with PyTorch can be done in the following way.\n",
    "\n",
    "- We specify a class MLP that extends PyTorch's nn.Module class. In other words, it's a neural network using PyTorch.\n",
    "- To the class, we add a def called compute_l1_loss. This is an implementation of taking the absolute value and summing all values for w in a particular trainable parameter.\n",
    "- In the training loop specified subsequently, we specify a L1 weight, collect all parameters, compute L1 loss, and add it to the loss function before error backpropagation.\n",
    "- We also print the L1 component of our loss when printing statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f658cb-0c10-4be1-b7da-49e9a564854d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 65.43456 (of which 63.13525 L1 loss)\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 4.08177 (of which 1.77761 L1 loss)\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 2.94135 (of which 0.63876 L1 loss)\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 2.94091 (of which 0.63833 L1 loss)\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 2.94062 (of which 0.63803 L1 loss)\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Prepare MNIST dataset\n",
    "    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=6)\n",
    "  \n",
    "    # Initialize the MLP\n",
    "    mlp = MLP()\n",
    "  \n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(mlp.parameters(), lr=1e-4)\n",
    "  \n",
    "    # Run the training loop\n",
    "    for epoch in range(0, 5): # 5 epochs at maximum\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "      \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "      \n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs)\n",
    "      \n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "      \n",
    "            # Compute L1 loss component\n",
    "            l1_weight = 1.0\n",
    "            l1_parameters = []\n",
    "            for parameter in mlp.parameters():\n",
    "                l1_parameters.append(parameter.view(-1))\n",
    "            l1 = l1_weight * mlp.compute_l1_loss(torch.cat(l1_parameters))\n",
    "      \n",
    "            # Add L1 loss component\n",
    "            loss += l1\n",
    "      \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "      \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "      \n",
    "            # Print statistics\n",
    "            minibatch_loss = loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print('Loss after mini-batch %5d: %.5f (of which %.5f L1 loss)'%(i + 1, minibatch_loss, l1))\n",
    "                #current_loss = 0.0\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a25fa3-780a-494e-925c-e79b2eea2f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
