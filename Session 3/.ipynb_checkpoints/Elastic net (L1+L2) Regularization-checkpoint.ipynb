{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ca3d61-c88a-4eac-bc4c-005d2acc66fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5778e-f2da-49fb-ac90-b7b4e0f74fb6",
   "metadata": {},
   "source": [
    "It is also possible to perform **Elastic Net Regularization** with PyTorch. This type of regularization essentially computes a weighted combination of L1 and L2 loss, with the weights of both summing to 1.0. In other words, we add $\\lambda_{L1} \\times \\sum_f{ _{i=1}^{n}} | w_i | + \\lambda_{L2} \\times \\sum_f{ _{i=1}^{n}} w_i^2$ to the loss component:\n",
    "\n",
    "$\\text{full_loss = original_loss + } \\lambda_{L1} \\times \\sum_f{ _{i=1}^{n}} | w_i | + \\lambda_{L2} \\times \\sum_f{ _{i=1}^{n}} w_i^2 $\n",
    "\n",
    "In this example, Elastic Net (L1 + L2) Regularization is implemented with PyTorch:\n",
    "\n",
    "- You can see that the MLP class representing the neural network provides two defs which are used to compute L1 and L2 loss, respectively.\n",
    "- In the training loop, these are applied, in a weighted fashion (with weights of 0.3 and 0.7, respectively).\n",
    "- The loss components are also printed on-screen when the statistics are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a2b19d-0d17-4a2f-8e87-abbf2c5a4bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(28 * 28 * 1, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "  \n",
    "    def compute_l1_loss(self, w):\n",
    "        return torch.abs(w).sum()\n",
    "    \n",
    "    def compute_l2_loss(self, w):\n",
    "        return torch.square(w).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f658cb-0c10-4be1-b7da-49e9a564854d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 24.79191 (of which 19.75580 L1 loss; 2.73684 L2 loss)\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 3.17805 (of which 0.82837 L1 loss; 0.04483 L2 loss)\n",
      "Starting epoch 3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Prepare MNIST dataset\n",
    "    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=6)\n",
    "  \n",
    "    # Initialize the MLP\n",
    "    mlp = MLP()\n",
    "  \n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(mlp.parameters(), lr=1e-4)\n",
    "  \n",
    "    # Run the training loop\n",
    "    for epoch in range(0, 5): # 5 epochs at maximum\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "      \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "      \n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs)\n",
    "      \n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "      \n",
    "            # Specify L1 and L2 weights\n",
    "            l1_weight = 0.3\n",
    "            l2_weight = 0.7\n",
    "    \n",
    "            # Compute L1 and L2 loss component\n",
    "            parameters = []\n",
    "            for parameter in mlp.parameters():\n",
    "                parameters.append(parameter.view(-1))\n",
    "            l1 = l1_weight * mlp.compute_l1_loss(torch.cat(parameters))\n",
    "            l2 = l2_weight * mlp.compute_l2_loss(torch.cat(parameters))\n",
    "      \n",
    "            # Add L1 and L2 loss components\n",
    "            loss += l1\n",
    "            loss += l2\n",
    "      \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "      \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "      \n",
    "            # Print statistics\n",
    "            minibatch_loss = loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print('Loss after mini-batch %5d: %.5f (of which %.5f L1 loss; %0.5f L2 loss)' % (i + 1, minibatch_loss, l1, l2))\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd44ec-8824-48f1-b049-a855cd647d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
