{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ca3d61-c88a-4eac-bc4c-005d2acc66fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a2b19d-0d17-4a2f-8e87-abbf2c5a4bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(28 * 28 * 1, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "  \n",
    "    def compute_l2_loss(self, w):\n",
    "        return torch.square(w).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5778e-f2da-49fb-ac90-b7b4e0f74fb6",
   "metadata": {},
   "source": [
    "**L2 Regularization**, also called **Ridge Regularization**, involves adding the squared value of all weights to the loss value.\n",
    "\n",
    "Implementing L2 Regularization with PyTorch is also easy. Understand that in this case, we don't take the absolute value for the weight values, but rather their squares. In other words, we add $\\sum_f{ _{i=1}^{n}} w_i^2$ to the loss component. In the example below, you can find how L2 Regularization can be used with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f658cb-0c10-4be1-b7da-49e9a564854d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 6.91637 (of which 4.61870 l2 loss)\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 2.65406 (of which 0.34957 l2 loss)\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 2.31669 (of which 0.01494 l2 loss)\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 2.30337 (of which 0.00030 l2 loss)\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 2.30208 (of which 0.00007 l2 loss)\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Prepare MNIST dataset\n",
    "    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=6)\n",
    "  \n",
    "    # Initialize the MLP\n",
    "    mlp = MLP()\n",
    "  \n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(mlp.parameters(), lr=1e-4)\n",
    "  \n",
    "    # Run the training loop\n",
    "    for epoch in range(0, 5): # 5 epochs at maximum\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "      \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "      \n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs)\n",
    "      \n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "      \n",
    "            # Compute l2 loss component\n",
    "            l2_weight = 1.0\n",
    "            l2_parameters = []\n",
    "            for parameter in mlp.parameters():\n",
    "                l2_parameters.append(parameter.view(-1))\n",
    "            l2 = l2_weight * mlp.compute_l2_loss(torch.cat(l2_parameters))\n",
    "      \n",
    "            # Add L2 loss component\n",
    "            loss += l2\n",
    "      \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "      \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "      \n",
    "            # Print statistics\n",
    "            minibatch_loss = loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print('Loss after mini-batch %5d: %.5f (of which %.5f l2 loss)' %(i + 1, minibatch_loss, l2))\n",
    "                #current_loss = 0.0\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2317eca-b14f-4e58-a822-a1a706c4014a",
   "metadata": {},
   "source": [
    "### Different way of adding L2 loss\n",
    "\n",
    "L2 based weight decay can also be implemented by setting a delta value for weight_decay in the optimizer. \n",
    "\n",
    "> weight_decay (float, optional) â€“ weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "For example: \n",
    "```\n",
    "optimizer = torch.optim.AdamW(mlp.parameters(), lr=1e-4, weight_decay=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd44ec-8824-48f1-b049-a855cd647d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
