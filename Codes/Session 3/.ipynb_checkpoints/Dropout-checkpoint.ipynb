{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce5778e-f2da-49fb-ac90-b7b4e0f74fb6",
   "metadata": {},
   "source": [
    "Models high in bias are relatively rigid. Linear models are a good example - they assume that your input data has a linear pattern. Models high in variance, however, do not make such assumptions -- but they are sensitive to changes in your training data. As you can imagine, striking a balance between rigidity and sensitivity.\n",
    "\n",
    "**Dropout** is related to the fact that deep neural networks have high variance. As you know when you have dealt with neural networks for a while, such models are sensitive to overfitting - capturing noise in the data as if it is part of the real function that must be modeled.\n",
    "\n",
    "> With Dropout, the training process essentially drops out neurons in a neural network.\n",
    "\n",
    "When certain neurons are dropped, no data flows through them anymore. Dropout is modeled as Bernoulli variables, which are either zero (0) or one (1). They can be configured with a variable, $p$, which illustrates the probability (between 0 and 1) with which neurons are dropped.\n",
    "\n",
    "When neurons are dropped, they are **not dropped permanently**: instead, at every epoch (or even minibatch) the network randomly selects neurons that are dropped this time. Neurons that had been dropped before can be activated again during future iterations.\n",
    "\n",
    "In the example, you'll see that:\n",
    "\n",
    "- We import a variety of dependencies. These include os for Python operating system interfaces, torch representing PyTorch, and a variety of sub components, such as its neural networks library (nn), the MNIST dataset, the DataLoader for loading the data, and transforms for a Tensor transform.\n",
    "- We define the MLP class, which is a PyTorch neural network module (nn.Module). Its constructor initializes the nn.Module super class and then initializes a Sequential network (i.e., a network where layers are stacked on top of each other). It begins by flattening the three-dimensional input (width, height, channels) into a one-dimensional input, then applies a Linear layer (MLP layer), followed by Dropout, Rectified Linear Unit. This is then repeated once more, before we end with a final Linear layer for the final multiclass prediction.\n",
    "- The forward definition is a relatively standard PyTorch definition that must be included in a nn.Module: it ensures that the forward pass of the network (i.e., when the data is fed to the network), is performed by feeding input data x through the layers defined in the constructor.\n",
    "- In the main check, a random seed is fixed, the dataset is loaded and prepared; the MLP, loss function and optimizer are initialized; then the model is trained. This is the classic PyTorch training loop: gradients are zeroed, a forward pass is performed, loss is computed and backpropagated through the network, and optimization is performed. Finally, after every iteration, statistics are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ca3d61-c88a-4eac-bc4c-005d2acc66fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a2b19d-0d17-4a2f-8e87-abbf2c5a4bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28 * 1, 64),      \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f658cb-0c10-4be1-b7da-49e9a564854d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 2.043\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 1.246\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 1.025\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.906\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.836\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Prepare MNIST dataset\n",
    "    dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=6)\n",
    "  \n",
    "    # Initialize the MLP\n",
    "    mlp = MLP()\n",
    "  \n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(mlp.parameters(), lr=1e-4, weight_decay=1.0)\n",
    "  \n",
    "    # Run the training loop\n",
    "    for epoch in range(0, 5): # 5 epochs at maximum\n",
    "    \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "      \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "      \n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs)\n",
    "      \n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "      \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "      \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "      \n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n",
    "                current_loss = 0.0\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd44ec-8824-48f1-b049-a855cd647d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
